---
title: 'Statistical Inference Course Project Part 1: Simulation Exercise'
author: "Edgar Bahilo Rodríguez"
date: "December 12 of 2018"
output:
  pdf_document: 
    fig_height: 12
    fig_width: 8
    number_sections: yes
    toc: yes
  html_document: default
---
#Introduction

This [R Markdown](http://rmarkdown.rstudio.com) notebook develops part 1 of the statistical inference project for the Statistical Inferece part of the Data Science Specialization by [Coursera](https://www.coursera.org) and [John Hopkins University](https://www.jhu.edu/). As it is described in the instructions of the assignment the purpose of this document is:

1. Investigate the exponential distribution given by the formula $\lambda e^{-\lambda x}$ in R, compare it with the Central Limit Theorem.
2. Illustrate via simulation and associated explanatory text the properties of the distribution of the mean of 40 exponentials.
    + Show the sample mean and compare it to the theoretical mean of the distribution.
    + Show how variable the sample is (via variance) and compare it to the theoretical variance of the distribution.
    + Show that the distribution is approximately normal.

#Simulations settings

The code displayed in the chunk below creates a vector with 4 different simulation sizes in order to study the effect of the CLT in a better way. Also the code defines the parameter of the exponential distribution $\lambda$ and the sample size $n$.
```{r include=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)
```

```{r}
set.seed(1234)
lambda<-0.2
n<-40
runs<-c(10,100,1000,10000)
```

##Simulations

The code below uses replicate (apply family) to create a matrix of size $n_{samples}* runs[i]$ under the exponential distribution described before. Then, the mean is calculated for each simulation of $n_{samples}$ for the different runs sizes of the simulations $[10,100,1000,1000]$

```{r}
simulations_10<-replicate(runs[1], rexp(n,lambda))
means_simul_10<-apply(simulations_10, 2, mean) #analogue can be done using for loops
simulations_100<-replicate(runs[2], rexp(n,lambda)) 
means_simul_100<-apply(simulations_100, 2, mean)
simulations_1000 <- replicate(runs[3], rexp(n, lambda))
means_simul_1000<-apply(simulations_1000, 2, mean)
simulations_10000 <- replicate(runs[4], rexp(n, lambda))
means_simul_10000<-apply(simulations_10000, 2, mean)
```


##Comparison: mean, variance, standard deviation and distribution

Now different statistics (mean of all the runs, variance of this mean and standard deviation of this mean) are calculated for each simulation size.

```{r}
mean_10<-mean(means_simul_10)
mean_100<-mean(means_simul_100)
mean_1000<-mean(means_simul_1000)
mean_10000<-mean(means_simul_10000)
var_10<-var(means_simul_10)
var_100<-var(means_simul_100)
var_1000<-var(means_simul_1000)
var_10000<-var(means_simul_10000)
sd_10<-sd(means_simul_10)
sd_100<-sd(means_simul_100)
sd_1000<-sd(means_simul_1000)
sd_10000<-sd(means_simul_10000)
mean_theoretical<-1/lambda
var_theoretical<-(1/lambda)^2/(n)
sd_theoretical<-1/(lambda * sqrt(n))
```

Then we compare the theoretical statistics (mean, variance and standard deviation) with the real ones:

```{r include=FALSE}
means_all<-cbind.data.frame(mean_10, mean_100, mean_1000, mean_10000, mean_theoretical)
var_all<-cbind.data.frame(var_10, var_100, var_1000, var_10000, var_theoretical)
sd_all<-cbind.data.frame(sd_10, sd_100, sd_1000, sd_10000, sd_theoretical)
```

```{r}
print(means_all) #code of each cbind is hiden to increase readability
print(var_all) #code of each cbind is hiden to increase readability
print(sd_all) #code of each cbind is hiden to increase readability

```

It can be seen how despite the similarities of the means with different simulation sizes, the variance and consequently the standard deviation look quite worse when the simulation runs are smaller.

As a matter of clarification, the distribution of different simulations sizes is showed in the plot grid displayed below. It is shown how as the run size increases the distribution looks more normal. At the end what we are doing when we increase the number of simulations is creating more samples, therefore allowing the data to converge according to the CLT.

```{r tidy=TRUE, echo=TRUE}
df_10 <- data.frame(means_simul_10)
df_100 <- data.frame(means_simul_100)
df_1000<- data.frame(means_simul_1000)
df_10000<- data.frame(means_simul_10000)

p1 <- ggplot(df_10, aes(x =means_simul_10))+geom_histogram(aes(y=..density..), colour="grey",
fill = "grey66")+ labs(title = "Distribution of means of 40 Samples for 10 simulations", x = "Mean of 40 Samples", y = "Density") + geom_vline(aes(xintercept = mean_10, colour = "sample"))+ geom_vline(aes(xintercept = mean_theoretical, colour = "theoretical")) + stat_function(fun = dnorm, args = list(mean = mean_10, sd = sd_10), color = "gold", size = 1.0) + stat_function(fun = dnorm, args = list(mean = mean_theoretical, sd = sd_theoretical), colour = "purple", size = 1.0)

p2 <- ggplot(df_100, aes(x =means_simul_100))+geom_histogram(aes(y=..density..), colour="grey",
fill = "grey66")+ labs(title = "Distribution of means of 40 Samples for 100 simulations", x = "Mean of 40 Samples", y = "Density") + geom_vline(aes(xintercept = mean_100, colour = "sample"))+ geom_vline(aes(xintercept = mean_theoretical, colour = "theoretical")) + stat_function(fun = dnorm, args = list(mean = mean_100, sd = sd_100), color = "gold", size = 1.0) + stat_function(fun = dnorm, args = list(mean = mean_theoretical, sd = sd_theoretical), colour = "purple", size = 1.0)

p3 <- ggplot(df_1000, aes(x =means_simul_1000))+geom_histogram(aes(y=..density..), colour="grey",
fill = "grey66")+ labs(title = "Distribution of means of 40 Samples for 1000 simulations", x = "Mean of 40 Samples", y = "Density") + geom_vline(aes(xintercept = mean_1000, colour = "sample"))+ geom_vline(aes(xintercept = mean_theoretical, colour = "theoretical")) + stat_function(fun = dnorm, args = list(mean = mean_1000, sd = sd_1000), color = "gold", size = 1.0) + stat_function(fun = dnorm, args = list(mean = mean_theoretical, sd = sd_theoretical), colour = "purple", size = 1.0)

p4 <- ggplot(df_10000, aes(x =means_simul_10000))+geom_histogram(aes(y=..density..), colour="grey",
fill = "grey66")+ labs(title = "Distribution of means of 40 Samples for 10000 simulations", x = "Mean of 40 Samples", y = "Density") + geom_vline(aes(xintercept = mean_10000, colour = "sample"))+ geom_vline(aes(xintercept = mean_theoretical, colour = "theoretical")) + stat_function(fun = dnorm, args = list(mean = mean_10000, sd = sd_10000), color = "gold", size = 1.0) + stat_function(fun = dnorm, args = list(mean = mean_theoretical, sd = sd_theoretical), colour = "purple", size = 1.0)

```

```{r echo=TRUE, warning=FALSE, paged.print=FALSE}
grid.arrange(p1,p2,p3,p4, ncol=1) #code of each plot is hided in order to increase readability, 
#purple line distribution is a normal distribution
#yellow line is the distribution of the simulated data
```

##Normality tests and QQplot

Finally, we want to ensure that the data is normal (despite of having this hindshight using the distribution plots). We use Shapiro-Wilk test to check the normality of the data obtained after 1000 simulations.

```{r echo=TRUE, fig.height=4, fig.width=6, warning=FALSE, paged.print=FALSE}
shapiro.test(means_simul_1000)
```

The test shows that we can not reject normality and the data looks normal with a significant p-value. We also plot the Q-Q plot for the data obtained after 10000 as Shapiro-Wilk can only deal with sample sizes lower than 5000.

```{r echo=TRUE, fig.height=4, fig.width=6, warning=FALSE, paged.print=FALSE}
qqnorm(means_simul_10000)
qqline(means_simul_10000, col = "red")
```
